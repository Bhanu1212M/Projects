{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhanu1212M/Projects/blob/main/Batch_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJKF2_zbD-Fj"
      },
      "source": [
        "# **Stock Price Prediction Using LSTM , SVM , Random Forest and GRU**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chc0w3XWD-Fm"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXR3oH7ED-Fn"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
        "from statsmodels.tsa.arima_process import ArmaProcess\n",
        "from statsmodels.tsa.stattools import pacf\n",
        "from statsmodels.regression.linear_model import yule_walker\n",
        "#from statsmodels.tsa.stattools import adfuller\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nHmtNUrnD-Fo"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
        "from statsmodels.tsa.arima_process import ArmaProcess\n",
        "from statsmodels.tsa.stattools import pacf\n",
        "from statsmodels.regression.linear_model import yule_walker\n",
        "#from statsmodels.tsa.stattools import adfuller\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "# Generate the data\n",
        "import numpy as np\n",
        "ar = np.array([1, -0.8, 0.2])\n",
        "ma = np.array([1])\n",
        "my_simulation = ArmaProcess(ar, ma).generate_sample(nsample=100)\n",
        "\n",
        "plt.figure(figsize=[10, 5]); # Set dimensions for figure\n",
        "plt.plot(my_simulation, linestyle='-', marker='o', color='b')\n",
        "plt.title(\"Simulated Process\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqtk2zuwD-Fp"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "from pandas.plotting import autocorrelation_plot\n",
        "plot_acf(my_simulation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDNDb9EeD-Fp"
      },
      "outputs": [],
      "source": [
        "plot_pacf(my_simulation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEz1dsZYD-Fq"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.arima_model import ARMA\n",
        "\n",
        "mod = sm.tsa.arima.ARIMA(my_simulation, order=(2, 0, 0))\n",
        "mod_fit = mod.fit()\n",
        "print(mod_fit.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75ZA5UrxD-Fq"
      },
      "outputs": [],
      "source": [
        "array = np.random.random(20) #.astype(np.float32\n",
        "array.reshape((1,20,1))\n",
        "array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFonfRFpD-Fq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[10, 5]); # Set dimensions for figure\n",
        "plt.plot(array, linestyle='-', marker='o', color='b')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naBsi8bdD-Fr"
      },
      "outputs": [],
      "source": [
        "array.reshape((4,5,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH-_XdGjD-Fr"
      },
      "outputs": [],
      "source": [
        "array.reshape((2,5,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VcEFEikD-Fs"
      },
      "outputs": [],
      "source": [
        "# Example 1\n",
        "array = np.random.random(20) #.astype(np.float32\n",
        "array.reshape((1,20,1))\n",
        "\n",
        "# Example 2\n",
        "array.reshape((4,5,1))\n",
        "\n",
        "# Example 3\n",
        "array.reshape((2,5,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7ov7OtgD-Fs"
      },
      "outputs": [],
      "source": [
        "# IF it is image data:\n",
        "# Create an RGB image that is 3 dimensional arrays of 8-bit unsigned integers.\n",
        "width = 5\n",
        "height = 4\n",
        "RGB = 3\n",
        "p = width * height * RGB\n",
        "img_data = np.random.randint(100,high=255, size=p, dtype=np.uint8) # Generate values in (100,255)\n",
        "img_data = img_data.reshape((height, width, RGB))\n",
        "img_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSUISqKtD-Ft"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img = Image.fromarray( img_data)\n",
        "img.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0mhphzfD-Ft"
      },
      "source": [
        "# **Importing Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUGyItNhD-Fu"
      },
      "source": [
        "Getting the Amazon stock data from yahoo financial API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqYyfZAvELkN"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtHNDmwuEPjW"
      },
      "outputs": [],
      "source": [
        "!pip install yahoofinancials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pQMVYoOD-Fu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from yahoofinancials import YahooFinancials\n",
        "AMZN = yf.download('AMZN',\n",
        "                      start='2013-01-01',\n",
        "                      end='2022-08-09',\n",
        "                      progress=False)\n",
        "# AMZN = yf.download('AMZN') for all\n",
        "all_data = AMZN[['Adj Close','Open', 'High', 'Low', 'Close', 'Volume']].round(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGJn3K0YD-Fu"
      },
      "source": [
        "### **Let's Understand more about the data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03HfvMXvD-Fv"
      },
      "outputs": [],
      "source": [
        "all_data.to_csv('stock_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk6zt87DD-Fv"
      },
      "outputs": [],
      "source": [
        "all_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQqewOsRD-Fw"
      },
      "outputs": [],
      "source": [
        "all_data.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRH6Mqt_D-Fw"
      },
      "source": [
        "# **EXploratory Data Analysis (EDA)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKVODZS2D-Fw"
      },
      "outputs": [],
      "source": [
        "all_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk10fZMAD-Fx"
      },
      "outputs": [],
      "source": [
        "all_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdgahau1D-Fx"
      },
      "source": [
        "#### **Let's Understand More About The Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fbs09gJD-Fx"
      },
      "outputs": [],
      "source": [
        "all_data.duplicated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvMRpKDND-Fx"
      },
      "outputs": [],
      "source": [
        "all_data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvoS06vUD-Fx"
      },
      "outputs": [],
      "source": [
        "#Getting all the columns\n",
        "print(\"Features of the dataset:\")\n",
        "all_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7sXhqyOD-Fy"
      },
      "outputs": [],
      "source": [
        "#check details about the data set\n",
        "print('Here is the information regarding the dataset :')\n",
        "all_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrvUb1B9D-Fy"
      },
      "outputs": [],
      "source": [
        "#print the unique value\n",
        "print('Here is the unique values in our dataset')\n",
        "all_data.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onubMAxjD-Fy"
      },
      "outputs": [],
      "source": [
        "#printign the data types of our data\n",
        "print('Here is the data types of the dataset :')\n",
        "all_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qviNGF-gD-Fy"
      },
      "outputs": [],
      "source": [
        "#Looking for the description of the dataset to get insights of the data\n",
        "all_data.describe(include='all').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr2dys2wD-Fy"
      },
      "outputs": [],
      "source": [
        "#check for count of missing values in each column.\n",
        "print('Here are the details of missing value details in our dataet:')\n",
        "all_data.isna().sum()\n",
        "all_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFcuVH8DD-Fz"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "# Calculate the number of missing values in each column\n",
        "missing_values = df.isna().sum()\n",
        "\n",
        "# Create a point plot for the missing values\n",
        "sns.pointplot(x=missing_values.index, y=missing_values.values)\n",
        "plt.title('Missing Values in DataFrame')\n",
        "plt.xlabel('Column')\n",
        "plt.ylabel('Number of Missing Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njbwm3qMD-Fz"
      },
      "outputs": [],
      "source": [
        "all_data['Close'].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuC9Z13XD-Fz"
      },
      "outputs": [],
      "source": [
        "new_var = all_data['Adj Close'].plot()\n",
        "new_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQhcEHZMD-Fz"
      },
      "outputs": [],
      "source": [
        "all_data['Adj Close'].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3qaCzn1D-Fz"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=all_data.Open)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8c_JtujD-F0"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('stock_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3l5wyQoD-F0"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY447hKyD-F0"
      },
      "source": [
        "#### **Seasonality Plot for each column in our data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuoAInk5D-F0"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb_bIF5yD-F0"
      },
      "outputs": [],
      "source": [
        "decompose_result = seasonal_decompose(data.Open, model='multiplicative', period=1)\n",
        "decompose_result.plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzbduvlQD-F0"
      },
      "outputs": [],
      "source": [
        "decompose_result = seasonal_decompose(data.Close, model='multiplicative', period=1)\n",
        "decompose_result.plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwGN2ijlD-F1"
      },
      "outputs": [],
      "source": [
        "decompose_result = seasonal_decompose(data.High, model='multiplicative', period=1)\n",
        "decompose_result.plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WGHr2YsD-F1"
      },
      "outputs": [],
      "source": [
        "decompose_result = seasonal_decompose(data.Low, model='multiplicative', period=1)\n",
        "decompose_result.plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DFf-4xUD-F1"
      },
      "outputs": [],
      "source": [
        "decompose_result = seasonal_decompose(data['Adj Close'], model='multiplicative', period=1)\n",
        "decompose_result.plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4shzBIHaD-F1"
      },
      "outputs": [],
      "source": [
        "decompose_result = seasonal_decompose(data['Volume'], model='multiplicative', period=1)\n",
        "decompose_result.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSI53lJJD-F1"
      },
      "source": [
        "**As We can see from the above there is no seasonality in our data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5KUejx4D-F2"
      },
      "source": [
        "### **Distribution Plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGVzRunGD-F2"
      },
      "outputs": [],
      "source": [
        "sns.distplot(data['Adj Close'].dropna(), color='purple');\n",
        "plt.ylabel('Daily Return');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1cXcAQJD-F2"
      },
      "outputs": [],
      "source": [
        "sns.distplot(x=all_data.Open)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jfnYy-SD-F2"
      },
      "outputs": [],
      "source": [
        "sns.distplot(x=all_data.Close)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqpgaTByD-F2"
      },
      "outputs": [],
      "source": [
        "sns.distplot(x=all_data.High)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wvCGTglD-F3"
      },
      "outputs": [],
      "source": [
        "sns.distplot(x=all_data.Close)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cGsaUstD-F3"
      },
      "outputs": [],
      "source": [
        "data.columns, data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyFfzRJTD-F3"
      },
      "source": [
        "### **Line Plots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M73XDy61D-F3"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.style.use('dark_background')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVjInfDED-F3"
      },
      "outputs": [],
      "source": [
        "fig = px.line(data,x = 'Date', y = ['High', 'Low'], template = 'plotly_dark')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUD1i3JgD-F4"
      },
      "outputs": [],
      "source": [
        "fig = px.line(data,x = 'Date', y = ['Open', 'Close'], template = 'plotly_dark')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4SMIZ_bD-F4"
      },
      "outputs": [],
      "source": [
        "fig = px.line(data,x = 'Date', y = ['Adj Close'], template = 'plotly_dark')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoJg1YzGD-F4"
      },
      "source": [
        "**From above plots we can conclude that:**\n",
        "\n",
        "**1. High - Increases over time**\n",
        "\n",
        "**2. Low - Increases over time**\n",
        "\n",
        "**3. Open - Increases over time**\n",
        "\n",
        "**4. Close - Increases over time**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOzkVpMJD-F4"
      },
      "outputs": [],
      "source": [
        "fig = px.line(data,x = 'Date', y = ['Volume'], template = 'plotly_dark')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b6uiE8hD-F4"
      },
      "source": [
        "**From above plot we can conclude that : Volume doesn't increase gradually but have non-linear decrement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU4lB5b9D-F5"
      },
      "source": [
        "### **Candle Stick Plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_geVxHQCD-F5"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Candlestick(x=data['Date'],\n",
        "                open=data['Open'],\n",
        "                high=data['High'],\n",
        "                low=data['Low'],\n",
        "                close=data['Close'] ,increasing_line_color= 'cyan', decreasing_line_color= 'gray')])\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tuJFjGBD-F5"
      },
      "source": [
        "**From above plot we can conclude that:Amazon's Stock increases over time and then have sudden shift in its increment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psaDsZZjD-F5"
      },
      "source": [
        "### **Joint Plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUMilUr7D-F5"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x='High', y='Low', data=all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FWC3OMTsD-F5"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x='Open', y='Close', data=all_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QrXiUhxD-F6"
      },
      "source": [
        "**From above plot we can conclude that:**\n",
        "\n",
        "\n",
        "**1. High and low are linarly dependent**\n",
        "\n",
        "\n",
        "**2. Open and close are linearly dependent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxrBJlrgD-F6"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x='Adj Close', y='Volume', data=all_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLvp2Z_qD-F6"
      },
      "source": [
        "**From above plot we can conclude that: For high values for VOLUME , Adj Close have low values so, they are inversely proportional.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vie_5PyqD-F6"
      },
      "source": [
        "### **Pairplot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adJ4lP3YD-F6"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data.drop('Date', axis =1), kind='reg');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMWIiOmfD-F6"
      },
      "source": [
        "### **Correlation Plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40uwaSbxD-F7"
      },
      "outputs": [],
      "source": [
        "fig = px.imshow(data.corr(), template = 'plotly_dark')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtqf_V58D-F7"
      },
      "source": [
        "**From the above figure we can see that except volume all columns are highly correlated with each other.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwL8qunuD-F7"
      },
      "source": [
        "### **Box Plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csZXFOXwD-F7"
      },
      "outputs": [],
      "source": [
        "fig = px.box(data,x=['Open', 'High', 'Low', 'Close', 'Adj Close'], template = 'plotly_dark',\n",
        "             title = 'Representation of Type of Stars with Temperature')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSAcQhEhD-F7"
      },
      "outputs": [],
      "source": [
        "fig = px.box(data,x=['Volume'], template = 'plotly_dark',\n",
        "             title = 'Representation of Type of Stars with Temperature')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeDHgLnPD-F8"
      },
      "source": [
        "### **3D - Scatter Plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxIGU80xD-F8"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_3d(data, x='High', y='Low', z='Close',\n",
        "              color='Adj Close', template = 'plotly_dark', title = 'Distribution of Highs, Lows and Closing Values represented by Adj Close')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWNFrlmRD-F8"
      },
      "outputs": [],
      "source": [
        "print('Ternary Scatter Plot')\n",
        "fig = px.scatter_ternary(data, a=\"High\", b=\"Low\", c=\"Close\",hover_name=\"Volume\",\n",
        "    color=\"Adj Close\", template = 'plotly_dark', size_max=30,)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aaBxXEJD-F8"
      },
      "outputs": [],
      "source": [
        "from matplotlib import dates\n",
        "df_month = all_data.resample(\"M\").mean()\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.xaxis.set_major_formatter(dates.DateFormatter('%Y-%m'))\n",
        "ax.bar(df_month['2020':].index, df_month.loc['2020':, \"Volume\"], width=25, align='center')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otaAqon_D-F8"
      },
      "source": [
        "### **Resampling and Rolling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amaoZknAD-F8"
      },
      "source": [
        "Resampling is very common in time-series data. Most of the time resampling is done to a lower frequency.Though resampling of higher frequency is also necessary especially for modeling purposes. Not so much in data analysis purpose. In the ‘Volume’ data we are working on right now, we can observe some big spikes here and there. These types of spikes are not helpful for data analysis or for modeling. normally to smooth out the spikes, resampling to a lower frequency and rolling is very helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnsGJyd5D-F9"
      },
      "outputs": [],
      "source": [
        "start, end = '2022-01', '2022-06'\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.plot(all_data.loc[start:end, 'Volume'],\n",
        "marker='.', linestyle='-', linewidth=0.5, label='Daily')\n",
        "ax.plot(df_month.loc[start:end, 'Volume'],\n",
        "marker='o', markersize=8, linestyle='-', label='Monthly Mean Resample')\n",
        "ax.set_ylabel('Volume')\n",
        "ax.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5NPpScED-F9"
      },
      "source": [
        "#### **Week Resample**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltjB9RLrD-F9"
      },
      "outputs": [],
      "source": [
        "df_week = all_data.resample(\"W\").mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii11LOfOD-F9"
      },
      "outputs": [],
      "source": [
        "start, end = '2022-01', '2022-08'\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.plot(all_data.loc[start:end, 'Volume'], marker='.', linestyle='-', linewidth = 0.5, label='Daily', color='white')\n",
        "ax.plot(df_week.loc[start:end, 'Volume'], marker='o', markersize=8, linestyle='-', label='Weekly', color='coral')\n",
        "ax.set_ylabel(\"Open\")\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-g0blj5D-F9"
      },
      "source": [
        "#### **Rolling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w5JIub6D-F-"
      },
      "source": [
        "Rolling is another very helpful way of smoothing out the curve. It takes the average of a specified amount of data. If I want a 7-day rolling, it gives us the 7-day average data.\n",
        "\n",
        "We are doing it on the above plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tLx1vGZD-F-"
      },
      "outputs": [],
      "source": [
        "df_7d_rolling = all_data.rolling(7, center=True).mean()\n",
        "start, end = '2021-06', '2022-05'\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.plot(all_data.loc[start:end, 'Volume'], marker='.', linestyle='-',\n",
        "        linewidth=0.5, label='Daily')\n",
        "ax.plot(df_week.loc[start:end, 'Volume'], marker='o', markersize=5,\n",
        "        linestyle='-', label = 'Weekly mean volume')\n",
        "ax.plot(df_7d_rolling.loc[start:end, 'Volume'], marker='.', linestyle='-', label='7d Rolling Average')\n",
        "ax.set_ylabel('Stock Volume')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-isDHrpzD-F-"
      },
      "source": [
        "7-d rolling average is a bit smoother than the weekly average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWHw7S1xD-F-"
      },
      "source": [
        "### **Differencing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbXXGdhYD-F-"
      },
      "source": [
        "Differencing takes the difference in values of a specified distance.It is a popular method to remove the trend in the data. The trend is not good for forecasting or modeling.\n",
        "\n",
        "I've used expanding window,an another way of transformation. It keeps adding the cumulative. For example, if you add an expanding function to the ‘High’ column first element remains the same. The second element becomes cumulative of the first and second element, the third element becomes cumulative of the first, second, and third element, and so on. You can use aggregate functions like mean, median, standard deviation, etc. on it too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuY3PmrCD-F-"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(20, 8))\n",
        "ax = all_data.High.plot(label='High')\n",
        "ax = all_data.High.expanding().mean().plot(label='High expanding mean')\n",
        "ax = all_data.High.expanding().std().plot(label='High expanding std')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "255jHP6JD-F_"
      },
      "source": [
        "### **Decomposition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiSkUbM7D-F_"
      },
      "source": [
        "**Decomposition will show the observations and these three elements in the same plot:**\n",
        "\n",
        "**1. Trend: Consistent upward or downward slope of a time series.**\n",
        "\n",
        "**2. Seasonality: Clear periodic pattern of a time series**\n",
        "\n",
        "**3. Noise: Outliers or missing values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHNOCq23D-F_"
      },
      "outputs": [],
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 11, 9\n",
        "decomposition = sm.tsa.seasonal_decompose(df_month['Volume'], model='Additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMeigoezD-G-"
      },
      "outputs": [],
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 11, 9\n",
        "decomposition = sm.tsa.seasonal_decompose(df_month['Open'], model='Additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbBS2LYDD-HB"
      },
      "outputs": [],
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 11, 9\n",
        "decomposition = sm.tsa.seasonal_decompose(df_month['Close'], model='Additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xcm97tWfD-HG"
      },
      "outputs": [],
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 11, 9\n",
        "decomposition = sm.tsa.seasonal_decompose(df_month['High'], model='Additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByUh-ogCD-HI"
      },
      "outputs": [],
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 11, 9\n",
        "decomposition = sm.tsa.seasonal_decompose(df_month['Low'], model='Additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8koHL5BDD-HI"
      },
      "outputs": [],
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 11, 9\n",
        "decomposition = sm.tsa.seasonal_decompose(df_month['Adj Close'], model='Additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldwQ1Rz2D-HI"
      },
      "source": [
        "### **Plotting The Change**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hyt39l-D-HI"
      },
      "source": [
        "#### **Shift**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V74lz5RfD-HJ"
      },
      "source": [
        "The shift function shifts the data before or after the specified amount of time. It will shift the data by one day by default. That means you will get the previous day's data. In financial data like this one, it is helpful to see previous day data and today's data side by side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeDWCvnkD-HJ"
      },
      "outputs": [],
      "source": [
        "df = all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSSMNhXSD-HJ"
      },
      "outputs": [],
      "source": [
        "df['Change'] = df.Close.div(df.Close.shift())\n",
        "df['Change'].plot(figsize=(20, 8), fontsize = 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMkm2tigD-HJ"
      },
      "source": [
        "In the code above, .div() helps to fill up the missing data. Actually, div() means division. df. div(6) will divide each element in df by 6. But here I used ‘df.Close.shift()’. So, Each element of df will be divided by each element of ‘df.Close.shift()’. We do this to avoid the null values that are created by the ‘shift()’ operation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU9d7Uh0D-HJ"
      },
      "source": [
        "### **Heat Map**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPaY-g3PD-HK"
      },
      "outputs": [],
      "source": [
        "corr = df.corr()\n",
        "\n",
        "corr.style.background_gradient(cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRJiuwjWD-HK"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbq5CIezD-HL"
      },
      "outputs": [],
      "source": [
        "print(\"There are \"+ str(all_data[:'2020'].shape[0]) + \" observations in the training data\")\n",
        "print(\"There are \"+ str(all_data['2021':].shape[0]) + \" observations in the test data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEgIO7OoD-HM"
      },
      "outputs": [],
      "source": [
        "def ts_train_test(all_data,time_steps,for_periods):\n",
        "    '''\n",
        "    input:\n",
        "      data: dataframe with dates and price data\n",
        "    output:\n",
        "      X_train, y_train: data from 2013/1/1-2020/12/31\n",
        "      X_test:  data from 2021 -\n",
        "      sc:      insantiated MinMaxScaler object fit to the training data\n",
        "    '''\n",
        "    # create training and test set\n",
        "    ts_train = all_data[:'2020'].iloc[:,0:1].values\n",
        "    ts_test  = all_data['2021':].iloc[:,0:1].values\n",
        "    ts_train_len = len(ts_train)\n",
        "    ts_test_len = len(ts_test)\n",
        "\n",
        "    # create training data of s samples and t time steps\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    y_train_stacked = []\n",
        "    for i in range(time_steps,ts_train_len-1):\n",
        "        X_train.append(ts_train[i-time_steps:i,0])\n",
        "        y_train.append(ts_train[i:i+for_periods,0])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "    # Reshaping X_train for efficient modelling\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
        "\n",
        "    inputs = pd.concat((all_data[\"Adj Close\"][:'2020'], all_data[\"Adj Close\"]['2021':]),axis=0).values\n",
        "    inputs = inputs[len(inputs)-len(ts_test) - time_steps:]\n",
        "    inputs = inputs.reshape(-1,1)\n",
        "\n",
        "    # Preparing X_test\n",
        "    X_test = []\n",
        "    for i in range(time_steps,ts_test_len+time_steps-for_periods):\n",
        "        X_test.append(inputs[i-time_steps:i,0])\n",
        "\n",
        "    X_test = np.array(X_test)\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
        "\n",
        "    return X_train, y_train , X_test\n",
        "\n",
        "X_train, y_train, X_test = ts_train_test(all_data,5,2)\n",
        "X_train.shape[0],X_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKQSFrPDD-HN"
      },
      "outputs": [],
      "source": [
        "# Make the 3-D shape to a data frame so we can see:\n",
        "X_train_see = pd.DataFrame(np.reshape(X_train, (X_train.shape[0],X_train.shape[1])))\n",
        "y_train_see = pd.DataFrame(y_train)\n",
        "pd.concat([X_train_see,y_train_see],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "814_AaykD-HN"
      },
      "outputs": [],
      "source": [
        "# Make the 3-D shape to a data frame so we can see:\n",
        "X_test_see = pd.DataFrame(np.reshape(X_test, (X_test.shape[0],X_test.shape[1])))\n",
        "pd.DataFrame(X_test_see)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xocGiM84D-HO"
      },
      "outputs": [],
      "source": [
        "print(\"There are \" + str(X_train.shape[0]) + \" samples in the training data\")\n",
        "print(\"There are \" + str(X_test.shape[0]) + \" samples in the test data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-7V3Fv1D-HO"
      },
      "source": [
        "# **Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3bsgwcID-HO"
      },
      "source": [
        "## **Simple RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3Yjhx-vD-HO"
      },
      "outputs": [],
      "source": [
        "def simple_rnn_model(X_train, y_train, X_test):\n",
        "    '''\n",
        "    create single layer rnn model trained on X_train and y_train\n",
        "    and make predictions on the X_test data\n",
        "    '''\n",
        "    # create a model\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense, SimpleRNN\n",
        "\n",
        "    my_rnn_model = Sequential()\n",
        "    my_rnn_model.add(SimpleRNN(32, return_sequences=True))\n",
        "    #my_rnn_model.add(SimpleRNN(32, return_sequences=True))\n",
        "    #my_rnn_model.add(SimpleRNN(32, return_sequences=True))\n",
        "    my_rnn_model.add(SimpleRNN(32))\n",
        "    my_rnn_model.add(Dense(2)) # The time step of the output\n",
        "\n",
        "    my_rnn_model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
        "\n",
        "    # fit the RNN model\n",
        "    my_rnn_model.fit(X_train, y_train, epochs=100, batch_size=150, verbose=0)\n",
        "\n",
        "    # Finalizing predictions\n",
        "    rnn_predictions = my_rnn_model.predict(X_test)\n",
        "\n",
        "    return my_rnn_model, rnn_predictions\n",
        "\n",
        "my_rnn_model, rnn_predictions = simple_rnn_model(X_train, y_train, X_test)\n",
        "rnn_predictions[1:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3z3oGudD-HO"
      },
      "outputs": [],
      "source": [
        "def actual_pred_plot(preds):\n",
        "    actual_pred = pd.DataFrame(columns = ['Adj. Close', 'prediction'])\n",
        "    actual_pred['Adj. Close'] = all_data.loc['2021':,'Adj Close'][0:len(preds)]\n",
        "    actual_pred['prediction'] = preds[:,0]\n",
        "\n",
        "    from keras.metrics import MeanSquaredError\n",
        "    m = MeanSquaredError()\n",
        "    m.update_state(np.array(actual_pred['Adj. Close']),np.array(actual_pred['prediction']))\n",
        "\n",
        "    return (m.result().numpy(), actual_pred.plot() )\n",
        "\n",
        "actual_pred_plot(rnn_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njlGMERJD-HP"
      },
      "outputs": [],
      "source": [
        "def ts_train_test_normalize(all_data,time_steps,for_periods):\n",
        "    '''\n",
        "    input:\n",
        "      data: dataframe with dates and price data\n",
        "    output:\n",
        "      X_train, y_train: data from 2013/1/1-2020/12/31\n",
        "      X_test:  data from 2021 -\n",
        "      sc:      insantiated MinMaxScaler object fit to the training data\n",
        "    '''\n",
        "    # create training and test set\n",
        "    ts_train = all_data[:'2020'].iloc[:,0:1].values\n",
        "    ts_test  = all_data['2021':].iloc[:,0:1].values\n",
        "    ts_train_len = len(ts_train)\n",
        "    ts_test_len = len(ts_test)\n",
        "\n",
        "    # scale the data\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    sc = MinMaxScaler(feature_range=(0,1))\n",
        "    ts_train_scaled = sc.fit_transform(ts_train)\n",
        "\n",
        "    # create training data of s samples and t time steps\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    y_train_stacked = []\n",
        "    for i in range(time_steps,ts_train_len-1):\n",
        "        X_train.append(ts_train_scaled[i-time_steps:i,0])\n",
        "        y_train.append(ts_train_scaled[i:i+for_periods,0])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "    # Reshaping X_train for efficient modelling\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
        "\n",
        "    inputs = pd.concat((all_data[\"Adj Close\"][:'2020'], all_data[\"Adj Close\"]['2021':]),axis=0).values\n",
        "    inputs = inputs[len(inputs)-len(ts_test) - time_steps:]\n",
        "    inputs = inputs.reshape(-1,1)\n",
        "    inputs  = sc.transform(inputs)\n",
        "\n",
        "    # Preparing X_test\n",
        "    X_test = []\n",
        "    for i in range(time_steps,ts_test_len+time_steps-for_periods):\n",
        "        X_test.append(inputs[i-time_steps:i,0])\n",
        "\n",
        "    X_test = np.array(X_test)\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
        "\n",
        "    return X_train, y_train , X_test, sc\n",
        "\n",
        "def simple_rnn_model(X_train, y_train, X_test, sc):\n",
        "    '''\n",
        "    create single layer rnn model trained on X_train and y_train\n",
        "    and make predictions on the X_test data\n",
        "    '''\n",
        "    # create a model\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense, SimpleRNN\n",
        "\n",
        "    my_rnn_model = Sequential()\n",
        "    my_rnn_model.add(SimpleRNN(32, return_sequences=True))\n",
        "    #my_rnn_model.add(SimpleRNN(32, return_sequences=True))\n",
        "    #my_rnn_model.add(SimpleRNN(32, return_sequences=True))\n",
        "    my_rnn_model.add(SimpleRNN(32))\n",
        "    my_rnn_model.add(Dense(2)) # The time step of the output\n",
        "\n",
        "    my_rnn_model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
        "\n",
        "    # fit the RNN model\n",
        "    my_rnn_model.fit(X_train, y_train, epochs=100, batch_size=150, verbose=0)\n",
        "\n",
        "    # Finalizing predictions\n",
        "    rnn_predictions = my_rnn_model.predict(X_test)\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    rnn_predictions = sc.inverse_transform(rnn_predictions)\n",
        "\n",
        "    return my_rnn_model, rnn_predictions\n",
        "\n",
        "\n",
        "X_train, y_train, X_test, sc = ts_train_test_normalize(all_data,5,2)\n",
        "my_rnn_model, rnn_predictions_2 = simple_rnn_model(X_train, y_train, X_test, sc)\n",
        "rnn_predictions_2[1:10]\n",
        "actual_pred_plot(rnn_predictions_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q6B9Z5bD-HP"
      },
      "source": [
        "## **Simple LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtZYmWv1GNHf"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m5eUD5pD-HP"
      },
      "outputs": [],
      "source": [
        "def LSTM_model(X_train, y_train, X_test, sc):\n",
        "    # create a model\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense, SimpleRNN, GRU, LSTM\n",
        "    from tensorflow.keras.optimizers.legacy import SGD\n",
        "\n",
        "    # The LSTM architecture\n",
        "    my_LSTM_model = Sequential()\n",
        "    my_LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
        "    my_LSTM_model.add(LSTM(units=50, activation='tanh'))\n",
        "    my_LSTM_model.add(Dense(units=2))\n",
        "\n",
        "    # Compiling\n",
        "    my_LSTM_model.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
        "    # Fitting to the training set\n",
        "    my_LSTM_model.fit(X_train,y_train,epochs=50,batch_size=150, verbose=1)\n",
        "\n",
        "    LSTM_prediction = my_LSTM_model.predict(X_test)\n",
        "    LSTM_prediction = sc.inverse_transform(LSTM_prediction)\n",
        "\n",
        "    return my_LSTM_model, LSTM_prediction\n",
        "\n",
        "my_LSTM_model, LSTM_prediction = LSTM_model(X_train, y_train, X_test, sc)\n",
        "LSTM_prediction[1:10]\n",
        "actual_pred_plot(LSTM_prediction)\n",
        "\n",
        "# Use the testing data to evaluate the accuracy of the model\n",
        "#LSTM_prediction.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
        "#print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP3N5B5pD-HP"
      },
      "source": [
        "## **Simple GRU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0idvgHUD-HQ"
      },
      "outputs": [],
      "source": [
        "def GRU_model(X_train, y_train, X_test, sc):\n",
        "    # create a model\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense, SimpleRNN, GRU\n",
        "    from tensorflow.keras.optimizers.legacy import SGD\n",
        "\n",
        "    # The GRU architecture\n",
        "    my_GRU_model = Sequential()\n",
        "    # First GRU layer with Dropout regularisation\n",
        "    my_GRU_model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
        "    #my_GRU_model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
        "    #my_GRU_model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
        "    my_GRU_model.add(GRU(units=50, activation='tanh'))\n",
        "    my_GRU_model.add(Dense(units=2))\n",
        "\n",
        "    # Compiling the RNN\n",
        "    my_GRU_model.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
        "    # Fitting to the training set\n",
        "    my_GRU_model.fit(X_train,y_train,epochs=50,batch_size=150, verbose=1)\n",
        "\n",
        "    GRU_prediction = my_GRU_model.predict(X_test)\n",
        "    GRU_prediction = sc.inverse_transform(GRU_prediction)\n",
        "\n",
        "    return my_GRU_model, GRU_prediction\n",
        "\n",
        "my_GRU_model, GRU_prediction = GRU_model(X_train, y_train, X_test, sc)\n",
        "GRU_prediction[1:10]\n",
        "actual_pred_plot(GRU_prediction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMHuf7Z0D-HQ"
      },
      "outputs": [],
      "source": [
        "GRU_prediction[1:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4tbqnfaD-HQ"
      },
      "source": [
        "## **Stacked LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ojf2DHiyD-HQ"
      },
      "outputs": [],
      "source": [
        "def LSTM_model_regularization(X_train, y_train, X_test, sc):\n",
        "    # create a model\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense, SimpleRNN, GRU, LSTM, Dropout\n",
        "    from tensorflow.keras.optimizers.legacy import SGD\n",
        "\n",
        "    # The LSTM architecture\n",
        "    my_LSTM_model = Sequential()\n",
        "    my_LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
        "    my_LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
        "    # my_LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
        "    my_LSTM_model.add(LSTM(units=50, activation='tanh'))\n",
        "    my_LSTM_model.add(Dropout(0.2))\n",
        "    my_LSTM_model.add(Dense(units=2))\n",
        "\n",
        "    # Compiling\n",
        "    my_LSTM_model.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
        "    # Fitting to the training set\n",
        "    my_LSTM_model.fit(X_train,y_train,epochs=50,batch_size=150, verbose=1)\n",
        "\n",
        "    LSTM_prediction = my_LSTM_model.predict(X_test)\n",
        "    LSTM_prediction = sc.inverse_transform(LSTM_prediction)\n",
        "\n",
        "    return my_LSTM_model, LSTM_prediction\n",
        "\n",
        "my_LSTM_model, LSTM_prediction = LSTM_model_regularization(X_train, y_train, X_test, sc)\n",
        "LSTM_prediction[1:10]\n",
        "actual_pred_plot(LSTM_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cGrBGk6D-HQ"
      },
      "source": [
        "## **Stacked GRU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_wOHjp9D-HQ"
      },
      "outputs": [],
      "source": [
        "def GRU_model_regularization(X_train, y_train, X_test, sc):\n",
        "    '''\n",
        "    create GRU model trained on X_train and y_train\n",
        "    and make predictions on the X_test data\n",
        "    '''\n",
        "    # create a model\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense, SimpleRNN, GRU\n",
        "    from tensorflow.keras.optimizers.legacy import SGD\n",
        "    from keras.layers import Dropout\n",
        "\n",
        "    # The GRU architecture\n",
        "    my_GRU_model = Sequential()\n",
        "    # First GRU layer with Dropout regularisation\n",
        "    my_GRU_model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
        "    my_GRU_model.add(Dropout(0.2))\n",
        "    # Second GRU layer\n",
        "    my_GRU_model.add(GRU(units=50, return_sequences=True, activation='tanh'))\n",
        "    my_GRU_model.add(Dropout(0.2))\n",
        "\n",
        "    # Third GRU layer\n",
        "    my_GRU_model.add(GRU(units=50, return_sequences=True, activation='tanh'))\n",
        "    my_GRU_model.add(Dropout(0.2))\n",
        "    # Fourth GRU layer\n",
        "    my_GRU_model.add(GRU(units=50, activation='tanh'))\n",
        "    my_GRU_model.add(Dropout(0.2))\n",
        "    # The output layer\n",
        "    my_GRU_model.add(Dense(units=2))\n",
        "    # Compiling the RNN\n",
        "    my_GRU_model.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
        "    # Fitting to the training set\n",
        "    my_GRU_model.fit(X_train,y_train,epochs=50,batch_size=150, verbose=1)\n",
        "\n",
        "    GRU_predictions = my_GRU_model.predict(X_test)\n",
        "    GRU_predictions = sc.inverse_transform(GRU_predictions)\n",
        "\n",
        "    return my_GRU_model, GRU_predictions\n",
        "\n",
        "my_GRU_model, GRU_predictions = GRU_model_regularization(X_train, y_train, X_test, sc)\n",
        "GRU_predictions[1:10]\n",
        "actual_pred_plot(GRU_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqDXEYU2ScVO"
      },
      "source": [
        "Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWAJIggUIP3A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X = np.random.rand(100, 28, 28) # Example 3D array\n",
        "y = np.random.randint(0, 2, 100) # Example target variable\n",
        "\n",
        "# Reshape X to a 2-dimensional array\n",
        "X_2d = X.reshape((X.shape[0], -1))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_2d, y, test_size=0.2)\n",
        "\n",
        "# Create the SVM model and fit it to the training data\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Use the testing data to evaluate the accuracy of the model\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMaIIpoYSlLu"
      },
      "source": [
        "Radom Forest Classfier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9pMZdXZSaEI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.random.rand(100, 28, 28) # Example 3D array\n",
        "y = np.random.randint(0, 2, 100) # Example target variable\n",
        "\n",
        "# Reshape X to a 2-dimensional array\n",
        "X_2d = X.reshape((X.shape[0], -1))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_2d, y, test_size=0.2)\n",
        "\n",
        "# Create the SVM model and fit it to the training data\n",
        "rf = RandomForestClassifier(n_estimators=52)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Use the testing data to evaluate the accuracy of the model\n",
        "accuracy = rf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg0ZBAq2htEH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.6.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "f4e9cda46bb2d9d7fe6ecdff0f8336a934348bf06cb492f2f42f60739b3403b4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}